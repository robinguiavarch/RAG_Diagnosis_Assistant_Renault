"""
Construction index FAISS pour Knowledge Graph Dense S&C avec connexion Cloud/Local
Index bas√© sur les textes combin√©s sympt√¥me + cause
Pour lancer:
docker run --rm \
  -v $(pwd)/.env:/app/.env \
  -v $(pwd)/data:/app/data \
  -v $(pwd)/config:/app/config \
  -v $(pwd)/pipeline_step:/app/pipeline_step \
  --network host \
  diagnosis-app \
  poetry run python pipeline_step/knowledge_graph_setup/build_symptom_vector_index_kg_dense_sc.py
"""

import os
import pickle
import faiss
import numpy as np
from neo4j import GraphDatabase
from sentence_transformers import SentenceTransformer
from dotenv import load_dotenv
import yaml

load_dotenv()

def load_settings():
    """Charge la configuration depuis settings.yaml"""
    script_dir = os.path.dirname(os.path.abspath(__file__))
    config_path = os.path.join(script_dir, "..", "..", "config", "settings.yaml")
    with open(config_path, 'r', encoding='utf-8') as file:
        return yaml.safe_load(file)

def get_neo4j_connection(kg_type="dense_sc"):
    """
    üåê Connexion intelligente Cloud/Local
    kg_type: "dense", "sparse", ou "dense_sc"
    """
    load_dotenv()
    
    # Priorit√© au Cloud si activ√©
    cloud_enabled = os.getenv("NEO4J_CLOUD_ENABLED", "false").lower() == "true"
    
    if cloud_enabled:
        print(f"üåê MODE CLOUD pour {kg_type.upper()}")
        
        if kg_type == "dense":
            uri = os.getenv("NEO4J_DENSE_CLOUD_URI")
            password = os.getenv("NEO4J_DENSE_CLOUD_PASS")
        elif kg_type == "sparse":
            uri = os.getenv("NEO4J_SPARSE_CLOUD_URI")
            password = os.getenv("NEO4J_SPARSE_CLOUD_PASS")
        elif kg_type == "dense_sc":
            uri = os.getenv("NEO4J_DENSE_SC_CLOUD_URI")
            password = os.getenv("NEO4J_DENSE_SC_CLOUD_PASS")
        
        if uri and password:
            print(f"üîå Connexion Cloud {kg_type}: {uri}")
            return GraphDatabase.driver(uri, auth=("neo4j", password))
        else:
            print(f"‚ùå Credentials cloud manquants pour {kg_type}")
            cloud_enabled = False
    
    # Fallback Local
    print(f"üè† MODE LOCAL pour {kg_type.upper()}")
    
    if kg_type == "dense":
        uri = os.getenv("NEO4J_URI_DENSE", "bolt://host.docker.internal:7687")
        user = os.getenv("NEO4J_USER_DENSE", "neo4j")
        password = os.getenv("NEO4J_PASS_DENSE", "password")
    elif kg_type == "sparse":
        uri = os.getenv("NEO4J_URI_SPARSE", "bolt://host.docker.internal:7689")
        user = os.getenv("NEO4J_USER_SPARSE", "neo4j")
        password = os.getenv("NEO4J_PASS_SPARSE", "password")
    elif kg_type == "dense_sc":
        uri = os.getenv("NEO4J_URI_DENSE_SC", "bolt://host.docker.internal:7690")
        user = os.getenv("NEO4J_USER_DENSE_SC", "neo4j")
        password = os.getenv("NEO4J_PASS_DENSE_SC", "password")
    
    print(f"üîå Connexion Local {kg_type}: {uri}")
    return GraphDatabase.driver(uri, auth=(user, password))

def build_symptom_index_dense_sc():
    """
    Construit l'index FAISS pour les sympt√¥mes Dense S&C
    Sauvegarde dans data/knowledge_base/symptom_embeddings_dense_s&c/
    """
    print("üöÄ Construction de l'index FAISS pour Dense S&C (Sympt√¥me + Cause)...")
    
    # === CONFIGURATION ===
    config = load_settings()
    
    # üÜï CONNEXION CLOUD/LOCAL INTELLIGENTE
    driver = get_neo4j_connection("dense_sc")
    
    # Mod√®le d'embedding
    model_name = config["models"]["embedding_model"]
    print(f"üì¶ Chargement du mod√®le : {model_name}")
    
    # Chemin de sortie pour les embeddings Dense S&C
    script_dir = os.path.dirname(os.path.abspath(__file__))
    output_dir = os.path.join(script_dir, "..", "..", "data", "knowledge_base", "symptom_embeddings_dense_s&c")

    try:
        # Test de connexion
        with driver.session() as test_session:
            test_session.run("RETURN 1")
        print("‚úÖ Connexion Neo4j Dense S&C r√©ussie")
        
        # === EXTRACTION DES DONN√âES S&C ===
        print("üìä Extraction des sympt√¥mes avec texte combin√©...")
        with driver.session() as session:
            result = session.run("""
                MATCH (s:Symptom)
                WHERE s.combined_text IS NOT NULL
                RETURN DISTINCT s.name AS symptom_name, 
                       s.combined_text AS combined_text,
                       s.equipment AS equipment
                ORDER BY s.name
            """)
            
            symptoms_data = []
            for record in result:
                symptoms_data.append({
                    'symptom_name': record["symptom_name"],
                    'combined_text': record["combined_text"],
                    'equipment': record["equipment"] or 'unknown'
                })
        
        print(f"‚úÖ {len(symptoms_data)} sympt√¥mes S&C extraits")
        
        if not symptoms_data:
            raise ValueError("‚ùå Aucun sympt√¥me S&C trouv√© dans la Knowledge Base!")
        
        # Extraction des textes pour embedding
        symptom_names = [s['symptom_name'] for s in symptoms_data]
        combined_texts = [s['combined_text'] for s in symptoms_data]
        
        print(f"üìù Exemples de textes combin√©s :")
        for i, text in enumerate(combined_texts[:3]):
            print(f"   {i+1}. {text}")
        
        # === G√âN√âRATION DES EMBEDDINGS S&C ===
        print("üß† G√©n√©ration des embeddings avec textes combin√©s...")
        model = SentenceTransformer(model_name)
        
        # üÜï EMBEDDING DES TEXTES COMBIN√âS (sympt√¥me + cause)
        embeddings = model.encode(
            combined_texts,  # Utilise les textes combin√©s, pas seulement les sympt√¥mes
            show_progress_bar=True, 
            normalize_embeddings=True,
            convert_to_numpy=True
        )
        
        print(f"‚úÖ Embeddings S&C g√©n√©r√©s : {embeddings.shape}")
        
        # === CONSTRUCTION DE L'INDEX FAISS ===
        print("üîß Construction de l'index FAISS S&C...")
        dim = embeddings.shape[1]
        index = faiss.IndexFlatIP(dim)  # Inner Product pour embeddings normalis√©s
        index.add(embeddings.astype('float32'))
        
        print(f"‚úÖ Index FAISS S&C cr√©√© avec {index.ntotal} vecteurs de dimension {dim}")
        
        # === SAUVEGARDE ===
        print(f"üíæ Sauvegarde dans : {output_dir}")
        os.makedirs(output_dir, exist_ok=True)
        
        # Sauvegarde de l'index FAISS
        index_path = os.path.join(output_dir, "index.faiss")
        faiss.write_index(index, index_path)
        print(f"‚úÖ Index FAISS sauvegard√© : {index_path}")
        
        # üÜï Sauvegarde des m√©tadonn√©es enrichies S&C
        metadata_path = os.path.join(output_dir, "symptom_embedding_dense_s&c.pkl")
        metadata = {
            'symptom_names': symptom_names,
            'combined_texts': combined_texts,  # üÜï Textes combin√©s
            'symptoms_data': symptoms_data,    # üÜï Donn√©es compl√®tes
            'model_name': model_name,
            'embedding_dim': dim,
            'total_symptoms': len(symptom_names),
            'source': 'knowledge_base_dense_s&c',
            'indexing_method': 'symptom_plus_cause_combined',  # üÜï M√©thode d'indexation
            'connection_mode': 'cloud' if os.getenv("NEO4J_CLOUD_ENABLED", "false").lower() == "true" else 'local'
        }
        
        with open(metadata_path, "wb") as f:
            pickle.dump(metadata, f)
        print(f"‚úÖ M√©tadonn√©es S&C sauvegard√©es : {metadata_path}")
        
        # === STATISTIQUES FINALES ===
        unique_symptoms = len(set(symptom_names))
        unique_equipments = set(s['equipment'] for s in symptoms_data)
        
        print("\nüìà STATISTIQUES DE L'INDEX DENSE S&C :")
        print(f"   ‚Ä¢ Sympt√¥mes index√©s : {len(symptom_names)}")
        print(f"   ‚Ä¢ Sympt√¥mes uniques : {unique_symptoms}")
        print(f"   ‚Ä¢ M√©thode : Sympt√¥me + Cause combin√©s")
        print(f"   ‚Ä¢ Dimension des embeddings : {dim}")
        print(f"   ‚Ä¢ Mod√®le utilis√© : {model_name}")
        print(f"   ‚Ä¢ Source : Knowledge Base Dense S&C")
        print(f"   ‚Ä¢ Mode connexion : {metadata['connection_mode'].upper()}")
        print(f"   ‚Ä¢ Taille index FAISS : {os.path.getsize(index_path) / 1024 / 1024:.2f} MB")
        
        # Equipements couverts
        print(f"   ‚Ä¢ √âquipements couverts : {len(unique_equipments)}")
        for eq in sorted(unique_equipments):
            count = sum(1 for s in symptoms_data if s['equipment'] == eq)
            print(f"     - {eq}: {count} sympt√¥mes")
        
        print(f"\nüìÅ Fichiers g√©n√©r√©s :")
        print(f"     - {index_path}")
        print(f"     - {metadata_path}")
        
        print("\nüéØ UTILISATION :")
        print("   Cet index peut maintenant √™tre utilis√© pour la recherche vectorielle")
        print("   enrichie par sympt√¥me + cause dans le pipeline RAG Dense S&C.")
        print("   La recherche sera plus contextuelle gr√¢ce √† l'enrichissement s√©mantique.")
        
    except Exception as e:
        print(f"‚ùå ERREUR lors de la construction de l'index S&C : {str(e)}")
        raise
    finally:
        driver.close()
        print("üîå Connexion Neo4j ferm√©e")

def main():
    """Pipeline principal de construction de l'index FAISS Dense S&C"""
    print("üöÄ D√âMARRAGE DE LA CONSTRUCTION DE L'INDEX FAISS DENSE S&C")
    print("=" * 70)
    print("üìù Objectif : Index vectoriel Sympt√¥me + Cause combin√©s")
    print("üåê Support : Cloud/Local automatique")
    print("üéØ Sortie : data/knowledge_base/symptom_embeddings_dense_s&c/")
    print()
    
    try:
        build_symptom_index_dense_sc()
        print("\n‚úÖ CONSTRUCTION DE L'INDEX FAISS DENSE S&C TERMIN√âE !")
        
    except FileNotFoundError as e:
        print(f"‚ùå ERREUR : Fichier de configuration manquant : {str(e)}")
    except Exception as e:
        print(f"‚ùå ERREUR INATTENDUE : {str(e)}")

if __name__ == "__main__":
    main()