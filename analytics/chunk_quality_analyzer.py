import os
import json
import random
from pathlib import Path
from typing import Dict, Any, List
import sys
import yaml
sys.path.append(str(Path(__file__).parent.parent))

def load_settings() -> Dict[str, Any]:
    """Charge la configuration depuis settings.yaml"""
    config_path = Path(__file__).parent.parent / "config" / "settings.yaml"
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)

def load_all_chunks(chunk_dir: Path) -> List[Dict[str, Any]]:
    """Charge tous les chunks de tous les documents"""
    all_chunks = []
    chunk_files = list(chunk_dir.glob("*_chunks.json"))
    
    print(f"üîç Fichiers chunks trouv√©s: {len(chunk_files)}")
    for chunk_file in chunk_files:
        print(f"   üìÑ {chunk_file.name}")
    
    for chunk_file in chunk_files:
        with open(chunk_file, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        # DIAGNOSTIC: V√©rifier d'o√π viennent les chunks
        print(f"\nüìä Analyse de {chunk_file.name}:")
        if "source_stats" in data:
            print(f"   ‚úÖ Fichier moderne avec m√©tadonn√©es")
            print(f"   üìÑ Document source: {data.get('document_id', 'N/A')}")
            print(f"   üîß M√©thode chunking: {data.get('chunking_config', {}).get('method', 'N/A')}")
        else:
            print(f"   ‚ö†Ô∏è Fichier ancien sans m√©tadonn√©es")
        
        # Analyser quelques chunks pour d√©tecter les mots coll√©s
        sample_chunks = data["chunks"][:2] if "chunks" in data else []
        for i, chunk in enumerate(sample_chunks):
            text = chunk.get("text", "")
            words = text.split()
            long_words = [w for w in words if len(w) > 15]
            if long_words:
                print(f"   ‚ö†Ô∏è Chunk {i}: mots suspects ({len(long_words)}): {long_words[:3]}")
            else:
                print(f"   ‚úÖ Chunk {i}: texte semble propre")
        
        for chunk in data["chunks"]:
            chunk_info = {
                "document_id": data["document_id"],
                "chunk_id": chunk["chunk_id"],
                "text": chunk["text"],
                "word_count": chunk.get("word_count", len(chunk["text"].split())),
                "char_count": chunk.get("char_count", len(chunk["text"])),
                "source_file": chunk_file.name,
                "has_metadata": "source_stats" in data,
                "extraction_method": data.get("chunking_config", {}).get("method", "unknown")
            }
            all_chunks.append(chunk_info)
    
    return all_chunks

def display_chunk(chunk: Dict[str, Any], index: int):
    """Affiche un chunk de mani√®re format√©e avec diagnostic"""
    print(f"{'='*80}")
    print(f"CHUNK #{index + 1}")
    print(f"{'='*80}")
    print(f"üìÑ Document: {chunk['document_id']}")
    print(f"üî¢ Chunk ID: {chunk['chunk_id']}")
    print(f"üìä Stats: {chunk['word_count']} mots | {chunk['char_count']} caract√®res")
    print(f"üìÅ Fichier: {chunk['source_file']}")
    print(f"üîß M√©tadonn√©es: {'‚úÖ Oui' if chunk['has_metadata'] else '‚ùå Non'}")
    print(f"üîß M√©thode: {chunk['extraction_method']}")
    
    # Diagnostic qualit√© du texte
    text = chunk['text']
    words = text.split()
    long_words = [w for w in words if len(w) > 15]
    
    if long_words:
        print(f"‚ö†Ô∏è PROBL√àME: {len(long_words)} mots suspects d√©tect√©s!")
        print(f"   Exemples: {long_words[:5]}")
    else:
        print(f"‚úÖ QUALIT√â: Texte semble propre")
    
    print(f"{'‚îÄ'*80}")
    print("üìù CONTENU:")
    print(f"{'‚îÄ'*80}")
    
    # Afficher le texte avec retour √† la ligne pour lisibilit√©
    text_lines = chunk['text'].split('. ')
    for line in text_lines:
        if line.strip():
            print(f"   {line.strip()}{'.' if not line.endswith('.') else ''}")
    
    print(f"{'‚îÄ'*80}")
    print()

def display_chunks_statistics(all_chunks: List[Dict[str, Any]]):
    """Affiche des statistiques g√©n√©rales sur les chunks avec diagnostic"""
    if not all_chunks:
        print("‚ùå Aucun chunk trouv√©!")
        return
    
    word_counts = [chunk['word_count'] for chunk in all_chunks]
    char_counts = [chunk['char_count'] for chunk in all_chunks]
    
    # Documents uniques
    unique_docs = set(chunk['document_id'] for chunk in all_chunks)
    
    # Diagnostic de qualit√©
    chunks_with_metadata = [c for c in all_chunks if c['has_metadata']]
    chunks_with_long_words = []
    
    for chunk in all_chunks:
        words = chunk['text'].split()
        long_words = [w for w in words if len(w) > 15]
        if long_words:
            chunks_with_long_words.append(chunk)
    
    print(f"üìä STATISTIQUES G√âN√âRALES")
    print(f"{'='*50}")
    print(f"üìÑ Nombre de documents: {len(unique_docs)}")
    print(f"üî¢ Nombre total de chunks: {len(all_chunks)}")
    print(f"üìè Taille moyenne (mots): {sum(word_counts) / len(word_counts):.1f}")
    print(f"üìè Taille min/max (mots): {min(word_counts)} / {max(word_counts)}")
    print()
    
    # Diagnostic de qualit√©
    print(f"üîç DIAGNOSTIC DE QUALIT√â")
    print(f"{'='*50}")
    print(f"‚úÖ Chunks avec m√©tadonn√©es: {len(chunks_with_metadata)}/{len(all_chunks)}")
    print(f"‚ö†Ô∏è Chunks avec mots suspects: {len(chunks_with_long_words)}/{len(all_chunks)}")
    
    if chunks_with_long_words:
        print(f"‚ùå PROBL√àME D√âTECT√â: Des chunks contiennent des mots coll√©s!")
        print(f"   Cela indique que l'extraction intelligente n'a pas √©t√© utilis√©e.")
        print(f"   V√©rifiez le chemin json_documents dans settings.yaml")
    else:
        print(f"‚úÖ QUALIT√â OK: Tous les chunks semblent propres")
    
    print()
    
    # Distribution par document
    doc_chunk_counts = {}
    for chunk in all_chunks:
        doc_id = chunk['document_id']
        doc_chunk_counts[doc_id] = doc_chunk_counts.get(doc_id, 0) + 1
    
    print("üìä R√âPARTITION PAR DOCUMENT:")
    print(f"{'‚îÄ'*50}")
    for doc_id, count in sorted(doc_chunk_counts.items()):
        print(f"   {doc_id}: {count} chunks")
    print()

def search_chunks_by_keyword(all_chunks: List[Dict[str, Any]], keyword: str) -> List[Dict[str, Any]]:
    """Recherche des chunks contenant un mot-cl√© (utile pour d√©bugger)"""
    matching_chunks = []
    keyword_lower = keyword.lower()
    
    for chunk in all_chunks:
        if keyword_lower in chunk['text'].lower():
            matching_chunks.append(chunk)
    
    return matching_chunks

def main():
    try:
        # Charger la configuration
        settings = load_settings()
        chunk_dir = Path(settings["paths"]["chunk_documents"])
        json_dir = Path(settings["paths"]["json_documents"])
        
        # DIAGNOSTIC: Afficher les chemins utilis√©s
        print(f"üîç DIAGNOSTIC DES CHEMINS:")
        print(f"{'='*50}")
        print(f"üìÅ JSON source: {json_dir}")
        print(f"üìÅ Chunks: {chunk_dir}")
        print(f"üìÅ JSON existe: {json_dir.exists()}")
        print(f"üìÅ Chunks existe: {chunk_dir.exists()}")
        
        if json_dir.exists():
            json_files = list(json_dir.glob("*.json"))
            print(f"üìÑ Fichiers JSON trouv√©s: {len(json_files)}")
            if json_files:
                print(f"üìÑ Premier fichier: {json_files[0].name}")
                
                # V√©rifier la qualit√© du premier fichier JSON
                with open(json_files[0], "r", encoding="utf-8") as f:
                    sample_data = json.load(f)
                
                if "pages" in sample_data and sample_data["pages"]:
                    sample_text = sample_data["pages"][0]["text"][:200]
                    sample_words = sample_text.split()
                    long_words = [w for w in sample_words if len(w) > 15]
                    
                    print(f"üîç Qualit√© du JSON source:")
                    if long_words:
                        print(f"   ‚ö†Ô∏è JSON contient des mots coll√©s: {long_words[:3]}")
                        print(f"   üí° Utilisez l'extraction intelligente: scripts/01_extract_text_PyMuPDF_intelligent.py")
                    else:
                        print(f"   ‚úÖ JSON semble propre")
        
        print()
        
        if not chunk_dir.exists():
            print(f"‚ùå Le r√©pertoire {chunk_dir} n'existe pas!")
            return
        
        print("üîç Chargement des chunks...")
        all_chunks = load_all_chunks(chunk_dir)
        
        if not all_chunks:
            print("‚ùå Aucun chunk trouv√© dans le r√©pertoire!")
            return
        
        # Afficher les statistiques avec diagnostic
        display_chunks_statistics(all_chunks)
        
        # S√©lectionner 3 chunks al√©atoires (r√©duit pour le diagnostic)
        num_samples = min(3, len(all_chunks))
        random_chunks = random.sample(all_chunks, num_samples)
        
        print(f"üé≤ AFFICHAGE DE {num_samples} CHUNKS AL√âATOIRES")
        print("="*80)
        print()
        
        for i, chunk in enumerate(random_chunks):
            display_chunk(chunk, i)
        
        # Recommandations bas√©es sur le diagnostic
        print("\n" + "="*80)
        print("üí° RECOMMANDATIONS")
        print("="*80)
        
        chunks_with_issues = [c for c in all_chunks if len([w for w in c['text'].split() if len(w) > 15]) > 0]
        
        if chunks_with_issues:
            print("‚ö†Ô∏è PROBL√àME D√âTECT√â:")
            print("   Vos chunks contiennent des mots coll√©s (texte mal extrait)")
            print()
            print("üîß SOLUTION:")
            print("   1. Utilisez l'extraction intelligente:")
            print("      poetry run python scripts/01_extract_text_PyMuPDF.py")
            print()
            print("   2. Modifiez settings.yaml pour pointer vers les JSONs:")
            print("      json_documents: data/json_documents/")
            print()
            print("   3. Re-chunkez avec le texte propre:")
            print("      poetry run python scripts/02_chunk_documents.py")
            print()
            print("   4. Re-visualisez:")
            print("      poetry run python visualization/visualize_chunks.py")
        else:
            print("‚úÖ QUALIT√â OK:")
            print("   Vos chunks semblent propres et pr√™ts pour le RAG!")
        
        print("\nüëã Diagnostic termin√©!")
        
    except Exception as e:
        print(f"‚ùå Erreur: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()