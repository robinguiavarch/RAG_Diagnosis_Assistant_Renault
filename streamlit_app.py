"""
RAG Comparator 4 Colonnes - Version Hybride ComplÃ¨te avec Multi-Query Fusion
Conserve TOUTE la logique sophistiquÃ©e + Interface 4 colonnes + Multi-Query + Equipment Matching
ğŸ†• NOUVEAU: Support Multi-Query avec processed_query pour les 3 gÃ©nÃ©rateurs KG
ğŸ†• AJOUT: Affichage du contexte envoyÃ© au LLM + Query utilisÃ©e dans les dÃ©tails techniques
Path: streamlit_app.py
"""

# IMPORTS STANDARDS
import yaml
import os
import time
import math
from dotenv import load_dotenv

# === CONFIGURATION STREAMLIT EN PREMIER ===
import streamlit as st
st.set_page_config(page_title="RAG Comparator 4 Columns", layout="wide")

# === IMPORTS DU PROJET APRÃˆS LA CONFIG STREAMLIT ===
from core.retrieval_engine.lexical_search import BM25Retriever
from core.retrieval_engine.semantic_search import FAISSRetriever
from core.retrieval_engine.hybrid_fusion import fuse_results
from core.reranking_engine.cross_encoder_reranker import CrossEncoderReranker
from core.response_generation.standard_rag_generator import OpenAIGenerator
from core.response_generation.rag_with_kg_dense_generator import OpenAIGeneratorKG
from core.response_generation.rag_with_kg_sparse_generator import OpenAIGeneratorKGSparse
from core.response_generation.rag_with_kg_dense_sc_generator import OpenAIGeneratorKGDenseSC

# ğŸ†• IMPORTS LLM Filtre et Juge
from core.query_processing import (
    create_query_processor, 
    create_enhanced_retrieval_engine
)
from core.evaluation import create_response_evaluator

# ğŸ†• IMPORTS DES NOUVELLES FONCTIONS KG AVEC MULTI-QUERY + EQUIPMENT MATCHING
from core.retrieval_graph.dense_kg_querier import (
    get_structured_context, 
    get_structured_context_with_equipment_filter
)
from core.retrieval_graph.sparse_kg_querier import (
    get_structured_context_sparse, 
    get_structured_context_sparse_with_equipment_filter
)
from core.retrieval_graph.dense_sc_kg_querier import (
    get_structured_context_dense_sc, 
    get_structured_context_dense_sc_with_equipment_filter
)

# === TITRE ET DESCRIPTION ===
st.title("ğŸ§  RAG Comparator â€“ 4 SystÃ¨mes avec Multi-Query Fusion + Equipment Matching")
st.markdown("**ğŸ†• Version Multi-Query** - Comparez RAG Classique, KG Dense, KG Sparse et KG Dense S&C avec LLM preprocessing intelligent")

# Status cloud
cloud_enabled = os.getenv("NEO4J_CLOUD_ENABLED", "false").lower() == "true"
if cloud_enabled:
    st.success("ğŸŒ Mode Cloud Neo4j activÃ©")
else:
    st.info("ğŸ  Mode Local Neo4j")

# === Chargement environnement et config ===
load_dotenv()

@st.cache_data
def load_settings():
    with open("config/settings.yaml", "r") as f:
        return yaml.safe_load(f)

settings = load_settings()
paths = settings["paths"]
models = settings["models"]
rerank_cfg = settings["reranking"]
gen_cfg = settings["generation"]

# === Chargement des composants ===
@st.cache_resource
def load_retrievers():
    """Charge les retrievers BM25 et FAISS avec les bons chemins"""
    try:
        # ğŸ”§ CORRECTION : Utilisation des bons chemins depuis settings.yaml
        bm25 = BM25Retriever(index_dir=paths["bm25_index"])
        
        # ğŸ”§ CORRECTION : Chemins FAISS classiques (pour RAG standard)
        faiss_index_path = paths["faiss_index"]  # data/indexes/semantic_faiss/index.faiss
        faiss_metadata_path = paths["embedding_file"]  # data/indexes/embeddings/metadata.pkl
        
        # Si les fichiers n'existent pas, essayer les chemins alternatifs
        if not os.path.exists(faiss_metadata_path):
            # Fallback vers le dossier FAISS
            alternative_metadata = os.path.join(paths["faiss_index_dir"], "metadata.pkl")
            if os.path.exists(alternative_metadata):
                faiss_metadata_path = alternative_metadata
                print(f"ğŸ”„ Utilisation du metadata alternatif: {alternative_metadata}")
            else:
                raise FileNotFoundError(f"âŒ MÃ©tadonnÃ©es FAISS non trouvÃ©es. VÃ©rifiez:\n"
                                      f"  - {faiss_metadata_path}\n"
                                      f"  - {alternative_metadata}\n"
                                      f"ExÃ©cutez d'abord le script de crÃ©ation des index FAISS.")
        
        faiss = FAISSRetriever(
            index_path=faiss_index_path, 
            metadata_path=faiss_metadata_path
        )
        
        print(f"âœ… Retrievers chargÃ©s:")
        print(f"   ğŸ“„ BM25: {paths['bm25_index']}")
        print(f"   ğŸ§  FAISS: {faiss_index_path}")
        print(f"   ğŸ“Š Metadata: {faiss_metadata_path}")
        
        return bm25, faiss
        
    except Exception as e:
        st.error(f"âŒ Erreur chargement retrievers: {e}")
        st.info("ğŸ’¡ Assurez-vous d'avoir crÃ©Ã© les index BM25 et FAISS avec les scripts appropriÃ©s")
        raise

@st.cache_resource
def load_reranker():
    """Charge le CrossEncoder local avec paramÃ¨tres optimisÃ©s"""
    return CrossEncoderReranker(
        model_name=models.get("reranker_model", "cross-encoder/ms-marco-MiniLM-L-6-v2"),
        max_length=1024
    )

@st.cache_resource
def load_generators():
    # 4 gÃ©nÃ©rateurs complets
    classic_generator = OpenAIGenerator(
        model=models.get("openai_model", "gpt-4o"),
        context_token_limit=6000,
        max_tokens=gen_cfg.get("max_new_tokens", 512),
    )
    
    kg_dense_generator = OpenAIGeneratorKG(
        model=models.get("openai_model", "gpt-4o"),
        context_token_limit=6000
    )
    
    kg_sparse_generator = OpenAIGeneratorKGSparse(
        model=models.get("openai_model", "gpt-4o"),
        context_token_limit=6000
    )
    
    kg_dense_sc_generator = OpenAIGeneratorKGDenseSC(
        model=models.get("openai_model", "gpt-4o"),
        context_token_limit=6000
    )
    
    return classic_generator, kg_dense_generator, kg_sparse_generator, kg_dense_sc_generator

# ğŸ†• Chargement du systÃ¨me LLM
@st.cache_resource
def load_llm_preprocessing():
    """Charge le systÃ¨me de prÃ©processing LLM"""
    try:
        query_processor = create_query_processor()
        
        bm25, faiss = load_retrievers()
        reranker = load_reranker()
        
        enhanced_retrieval = create_enhanced_retrieval_engine(
            bm25_retriever=bm25,
            faiss_retriever=faiss,
            reranker=reranker
        )
        
        return query_processor, enhanced_retrieval
        
    except Exception as e:
        st.error(f"âŒ Erreur chargement LLM preprocessing: {e}")
        return None, None

# ğŸ†• Chargement de l'Ã©valuateur 4 rÃ©ponses
@st.cache_resource
def load_response_evaluator():
    """Charge le systÃ¨me d'Ã©valuation LLM 4 rÃ©ponses"""
    try:
        return create_response_evaluator()
    except Exception as e:
        st.error(f"âŒ Erreur chargement Ã©valuateur: {e}")
        return None

# Chargement des composants
bm25, faiss = load_retrievers()
reranker = load_reranker()
classic_generator, kg_dense_generator, kg_sparse_generator, kg_dense_sc_generator = load_generators()

# ğŸ†• Chargement du systÃ¨me LLM
query_processor, enhanced_retrieval = load_llm_preprocessing()

# ğŸ†• Chargement de l'Ã©valuateur 4 rÃ©ponses
response_evaluator = load_response_evaluator()

# === Fonction de reranking locale avec CrossEncoder (CONSERVÃ‰E) ===
def rerank_with_cross_encoder(query, docs, top_k=3):
    """Re-ranking local avec CrossEncoder - Remplace l'API HuggingFace"""
    try:
        reranked_docs = reranker.rerank(
            query=query,
            candidates=docs,
            top_k=top_k,
            return_scores=True
        )
        
        formatted_results = []
        for doc in reranked_docs:
            formatted_results.append({
                "text": doc["text"],
                "score": doc["cross_encoder_score"],
                "cross_encoder_score": doc["cross_encoder_score"],
                "fused_score": doc.get("fused_score", 0.0),
                "source": doc.get("source", "Unknown"),
                "original_rank": doc.get("original_rank", 0)
            })
        
        return formatted_results
        
    except Exception as e:
        print(f"âŒ Erreur lors du reranking CrossEncoder : {e}")
        fallback_docs = sorted(docs, key=lambda x: x.get("fused_score", 0), reverse=True)[:top_k]
        return [{"text": d["text"], "score": d.get("fused_score", 0.0), "source": d.get("source", "Unknown")} for d in fallback_docs]

# === Fonctions d'Ã©valuation existantes (CONSERVÃ‰ES) ===
def determine_strategy_info(reranked_docs, kg_triplets_text, seuil_pertinence):
    """DÃ©termine et formate l'information de stratÃ©gie pour l'affichage"""
    
    kg_has_content = "Triplet" in kg_triplets_text if kg_triplets_text else False
    triplet_count = len([line for line in kg_triplets_text.split('\n') if 'Triplet' in line]) if kg_triplets_text else 0
    
    if reranked_docs:
        max_raw_score = max([d.get("cross_encoder_score", 0) for d in reranked_docs])
        try:
            max_normalized_score = 1.0 / (1.0 + math.exp(-max_raw_score))
        except:
            max_normalized_score = 0.5
        
        doc_has_content = max_normalized_score >= seuil_pertinence
    else:
        max_normalized_score = 0.0
        doc_has_content = False
    
    if not doc_has_content and not kg_has_content:
        return {
            "strategy": "AUCUN_CONTEXTE",
            "text": "ğŸš« **StratÃ©gie :** AUCUN_CONTEXTE",
            "color": "red",
            "details": "Aucun contexte pertinent trouvÃ©"
        }
    elif not doc_has_content and kg_has_content:
        return {
            "strategy": "KG_SEULEMENT", 
            "text": f"ğŸ§  **StratÃ©gie :** KG_SEULEMENT ",
            "color": "blue",
            "details": f"Knowledge Graph uniquement "
        }
    elif doc_has_content and not kg_has_content:
        return {
            "strategy": "DOC_SEULEMENT",
            "text": f"ğŸ“„ StratÃ©gie : DOC_SEULEMENT ",
            "color": "green", 
            "details": "Documents de recherche uniquement"
        }
    else:
        return {
            "strategy": "HYBRIDE",
            "text": f"ğŸ”„ StratÃ©gie : HYBRIDE ",
            "color": "purple",
            "details": f"Documents + Knowledge Graph"
        }

# ğŸ†• FONCTIONS RAG 4 SYSTÃˆMES AVEC MULTI-QUERY FUSION
def run_rag_system(query, system_type, use_llm_preprocessing):
    """
    ğŸ†• ExÃ©cute un systÃ¨me RAG donnÃ© avec Multi-Query Fusion si LLM preprocessing activÃ©
    
    Args:
        query: Question utilisateur
        system_type: Type de systÃ¨me ("classic", "dense", "sparse", "dense_sc")
        use_llm_preprocessing: Utiliser le LLM preprocessing + Multi-Query
    """
    
    # ğŸ†• Variables pour stocker les donnÃ©es LLM
    equipment_info = None
    processed_query = None  # ğŸ†• AJOUT CRITIQUE

    # ğŸ†• LLM PREPROCESSING UNIFORME POUR TOUS LES SYSTÃˆMES
    if use_llm_preprocessing and query_processor and enhanced_retrieval:
        # PIPELINE LLM PREPROCESSING + MULTI-QUERY
        try:
            print(f"ğŸ§  Activation Multi-Query pour systÃ¨me: {system_type}")
            
            # ğŸ†• STOCKAGE DE processed_query
            processed_query = query_processor.process_user_query(query)
            retrieval_result = enhanced_retrieval.search_with_variants(processed_query)
            reranked = retrieval_result.chunks
            processing_time = retrieval_result.processing_time
            
            # Stockage des mÃ©tadonnÃ©es complÃ¨tes
            reranked_metadata = [{"content": d["text"], "score": d.get("cross_encoder_score", d.get("fused_score", 0.0)), "source": d["source"]} for d in reranked]

            
            # ğŸ†• EXTRACTION EQUIPMENT_INFO POUR LES KG
            equipment_info = {
                'primary_equipment': processed_query.equipment_info.primary_equipment,
                'equipment_type': processed_query.equipment_info.equipment_type,
                'manufacturer': processed_query.equipment_info.manufacturer,
                'series': processed_query.equipment_info.series
            }
            
            print(f"âœ… Multi-Query activÃ©: {len(processed_query.query_variants)} variantes")
            print(f"ğŸ­ Equipment dÃ©tectÃ©: {equipment_info['primary_equipment']}")
            
            # RÃ©cupÃ©ration des triplets KG POUR TOUS (mÃªme si vide pour classique)
            kg_triplets_detailed = "\n".join([
                f"Triplet {i}: {t.get('symptom', '')} â†’ {t.get('cause', '')} â†’ {t.get('remedy', '')}"
                for i, t in enumerate(retrieval_result.triplets, 1)
            ]) if retrieval_result.triplets else ""
            
        except Exception as e:
            st.error(f"âŒ Erreur LLM preprocessing pour {system_type}: {e}")
            # Fallback vers pipeline classique
            use_llm_preprocessing = False
            equipment_info = None
            processed_query = None  # ğŸ†• RESET
    
    if not use_llm_preprocessing or not query_processor:
        # PIPELINE CLASSIQUE UNIFORME POUR TOUS LES 4 SYSTÃˆMES
        print(f"ğŸ“„ Mode classique pour systÃ¨me: {system_type}")
        start_time = time.time()
        
        # Recherche BM25 et FAISS (IDENTIQUE pour tous)
        bm25_raw = bm25.search(query, top_k=3)
        for doc in bm25_raw:
            doc["source"] = "Lexical (BM25)"

        faiss_raw = faiss.search(query, top_k=3)
        for doc in faiss_raw:
            doc["source"] = "SÃ©mantique (FAISS)"

        # Fusion et reranking local (IDENTIQUE pour tous)
        fused = fuse_results(bm25_raw, faiss_raw, top_k=rerank_cfg.get("top_k_before_rerank", 10))
        reranked = rerank_with_cross_encoder(query, fused, top_k=rerank_cfg.get("final_top_k", 3))
        
        processing_time = time.time() - start_time
        reranked_metadata = [{"content": d["text"], "score": d.get("cross_encoder_score", d.get("fused_score", 0.0)), "source": d["source"]} for d in reranked]

        kg_triplets_detailed = ""

   # ğŸ”§ DÃ‰FINIR LA QUERY EFFECTIVE UNE SEULE FOIS
    effective_query = processed_query.filtered_query if (use_llm_preprocessing and processed_query) else query

    # ğŸ†• STOCKAGE DU CONTEXTE POUR AFFICHAGE DEBUG
    document_context = "\n\n".join([d["text"] for d in reranked[:gen_cfg.get("max_context_chunks", 3)]])
    kg_context = ""

    # ğŸ†• GÃ‰NÃ‰RATION SELON LE SYSTÃˆME AVEC QUERY FILTRÃ‰E
    try:
        if system_type == "classic":
            # RAG Classique: SEULEMENT les documents, pas de KG
            answer = classic_generator.generate_answer(effective_query, [d["text"] for d in reranked])
            kg_context = "[Pas de contexte KG pour RAG Classique]"
            
        elif system_type == "dense":
            # ğŸ”§ CORRECTION: effective_query au lieu de query
            answer = kg_dense_generator.generate_answer(
                effective_query, [d["text"] for d in reranked], 
                reranked_metadata=reranked, 
                equipment_info=equipment_info,
                processed_query=processed_query if use_llm_preprocessing else None
            )
            # RÃ©cupÃ©ration du contexte KG pour affichage
            if processed_query and hasattr(processed_query, 'query_variants'):
                from core.retrieval_graph.dense_kg_querier import get_structured_context_with_multi_query
                kg_context = get_structured_context_with_multi_query(
                    processed_query.filtered_query, processed_query.query_variants, 
                    equipment_info or {}, format_type="detailed", max_triplets=3
                )
            elif equipment_info:
                kg_context = get_structured_context_with_equipment_filter(
                    effective_query, equipment_info, format_type="detailed", max_triplets=3
                )
            else:
                kg_context = get_structured_context(effective_query, format_type="detailed", max_triplets=3)
            
        elif system_type == "sparse":
            # ğŸ”§ CORRECTION: effective_query au lieu de query
            answer = kg_sparse_generator.generate_answer(
                effective_query, [d["text"] for d in reranked], 
                reranked_metadata=reranked,
                equipment_info=equipment_info,
                processed_query=processed_query if use_llm_preprocessing else None
            )
            # RÃ©cupÃ©ration du contexte KG pour affichage
            if processed_query and hasattr(processed_query, 'query_variants'):
                from core.retrieval_graph.sparse_kg_querier import get_structured_context_sparse_with_multi_query
                kg_context = get_structured_context_sparse_with_multi_query(
                    processed_query.filtered_query, processed_query.query_variants, 
                    equipment_info or {}, format_type="detailed", max_triplets=3
                )
            elif equipment_info:
                kg_context = get_structured_context_sparse_with_equipment_filter(
                    effective_query, equipment_info, format_type="detailed", max_triplets=3
                )
            else:
                kg_context = get_structured_context_sparse(effective_query, format_type="detailed", max_triplets=3)
            
        elif system_type == "dense_sc":
            # ğŸ”§ CORRECTION: effective_query au lieu de query
            answer = kg_dense_sc_generator.generate_answer(
                effective_query, [d["text"] for d in reranked], 
                reranked_metadata=reranked,
                equipment_info=equipment_info,
                processed_query=processed_query if use_llm_preprocessing else None
            )
            # RÃ©cupÃ©ration du contexte KG pour affichage
            if processed_query and hasattr(processed_query, 'query_variants'):
                from core.retrieval_graph.dense_sc_kg_querier import get_structured_context_dense_sc_with_multi_query
                kg_context = get_structured_context_dense_sc_with_multi_query(
                    processed_query.filtered_query, processed_query.query_variants, 
                    equipment_info or {}, format_type="detailed", max_triplets=3
                )
            elif equipment_info:
                kg_context = get_structured_context_dense_sc_with_equipment_filter(
                    effective_query, equipment_info, format_type="detailed", max_triplets=3
                )
            else:
                kg_context = get_structured_context_dense_sc(effective_query, format_type="detailed", max_triplets=3)
            
        else:
            answer = f"âŒ SystÃ¨me inconnu: {system_type}"

    except Exception as e:
        answer = f"âŒ Erreur gÃ©nÃ©ration {system_type}: {str(e)}"
        kg_context = f"âŒ Erreur rÃ©cupÃ©ration contexte KG: {str(e)}"

    # ğŸ†• RÃ‰CUPÃ‰RATION DES TRIPLETS KG APRÃˆS GÃ‰NÃ‰RATION POUR AFFICHAGE (si pas dÃ©jÃ  fait)
    if system_type in ["dense", "sparse", "dense_sc"] and not kg_triplets_detailed:
        try:
            # RÃ©cupÃ©ration pour affichage avec equipment matching si disponible
            if system_type == "dense":
                if equipment_info:
                    kg_triplets_detailed = get_structured_context_with_equipment_filter(
                        query, equipment_info, format_type="detailed", max_triplets=3
                    )
                else:
                    kg_triplets_detailed = get_structured_context(query, format_type="detailed", max_triplets=3)
                    
            elif system_type == "sparse":
                if equipment_info:
                    kg_triplets_detailed = get_structured_context_sparse_with_equipment_filter(
                        query, equipment_info, format_type="detailed", max_triplets=3
                    )
                else:
                    kg_triplets_detailed = get_structured_context_sparse(query, format_type="detailed", max_triplets=3)
                    
            elif system_type == "dense_sc":
                if equipment_info:
                    kg_triplets_detailed = get_structured_context_dense_sc_with_equipment_filter(
                        query, equipment_info, format_type="detailed", max_triplets=3
                    )
                else:
                    kg_triplets_detailed = get_structured_context_dense_sc(query, format_type="detailed", max_triplets=3)
        except Exception:
            kg_triplets_detailed = "Erreur rÃ©cupÃ©ration triplets KG"

    # StratÃ©gie pour les systÃ¨mes KG
    if system_type in ["dense", "sparse", "dense_sc"] and kg_triplets_detailed:
        strategy_info = determine_strategy_info(reranked_metadata, kg_triplets_detailed, gen_cfg.get("seuil_pertinence", 0.7))
    else:
        strategy_info = {"strategy": "RAG_CLASSIQUE", "text": "ğŸ“„ RAG Classique", "color": "blue"}

    # ğŸ†• AJOUT D'INFOS MULTI-QUERY DANS LA STRATÃ‰GIE
    if processed_query and use_llm_preprocessing:
        strategy_info["text"] += f" [Multi-Query: {len(processed_query.query_variants)} variantes]"

    return {
        "answer": answer,
        "time": processing_time,
        "docs_count": len(reranked),
        "reranked_docs": reranked_metadata,
        "kg_triplets": kg_triplets_detailed,
        "strategy": strategy_info,
        "equipment_info": equipment_info,  # ğŸ†• Ajout pour affichage
        "processed_query": processed_query,  # ğŸ†• Ajout pour diagnostics
        "effective_query": effective_query,  # ğŸ†• AJOUT: Query utilisÃ©e par le gÃ©nÃ©rateur
        "document_context": document_context,  # ğŸ†• AJOUT: Contexte documentaire envoyÃ© au LLM
        "kg_context": kg_context  # ğŸ†• AJOUT: Contexte KG envoyÃ© au LLM
    }

# === Interface utilisateur ===
query = st.text_area("ğŸ’¬ Votre requÃªte", height=100, 
                    placeholder="Ex: motor overheating FANUC R-30iB error ACAL-006")

# === Options d'affichage ===
col1, col2 = st.columns(2)
with col1:
    show_details = st.checkbox("Afficher les dÃ©tails techniques", value=False)
with col2:
    use_llm_preprocessing = st.checkbox("ğŸ§  Activer Multi-Query Fusion + Equipment Matching", value=True, 
                                       help="Utilise un LLM pour optimiser la requÃªte + Multi-Query sur les KG + dÃ©tection d'Ã©quipement")

if st.button("ğŸš€ Comparer les 4 systÃ¨mes", type="primary") and query.strip():
    
    # ğŸ†• EXÃ‰CUTION DES 4 SYSTÃˆMES AVEC MULTI-QUERY FUSION
    with st.spinner("âš¡ GÃ©nÃ©ration des 4 rÃ©ponses avec Multi-Query Fusion..."):
        # ExÃ©cution sÃ©quentielle pour garder la logique sophistiquÃ©e
        classic_result = run_rag_system(query, "classic", use_llm_preprocessing)
        dense_result = run_rag_system(query, "dense", use_llm_preprocessing)
        sparse_result = run_rag_system(query, "sparse", use_llm_preprocessing)
        dense_sc_result = run_rag_system(query, "dense_sc", use_llm_preprocessing)

    # ğŸ†• === AFFICHAGE MULTI-QUERY INFO SI LLM PREPROCESSING ACTIF ===
    if use_llm_preprocessing and dense_result.get("processed_query"):
        processed_query = dense_result["processed_query"]
        st.markdown("---")
        st.markdown("### ğŸ§  Multi-Query Fusion (LLM Preprocessing)")
        
        col_mq1, col_mq2, col_mq3 = st.columns(3)
        with col_mq1:
            st.info(f"**Query filtrÃ©e:** {processed_query.filtered_query}")
        with col_mq2:
            st.info(f"**Variantes:** {len(processed_query.query_variants)}")
        with col_mq3:
            if processed_query.query_variants:
                st.info(f"**PremiÃ¨re variante:** {processed_query.query_variants[0][:50]}...")
        
        # Affichage des variantes complÃ¨tes
        if show_details:
            with st.expander("ğŸ” DÃ©tails Multi-Query"):
                st.write("**Query originale:**", query)
                st.write("**Query filtrÃ©e:**", processed_query.filtered_query)
                st.write("**Variantes gÃ©nÃ©rÃ©es:**")
                for i, variant in enumerate(processed_query.query_variants, 1):
                    st.write(f"  {i}. {variant}")

    # ğŸ†• === AFFICHAGE EQUIPMENT INFO SI LLM PREPROCESSING ACTIF ===
    if use_llm_preprocessing and dense_result.get("equipment_info"):
        equipment_info = dense_result["equipment_info"]
        st.markdown("### ğŸ­ Equipment Detection (Multi-Query)")
        col_eq1, col_eq2, col_eq3, col_eq4 = st.columns(4)
        with col_eq1:
            st.info(f"**Equipment:** {equipment_info.get('primary_equipment', 'Unknown')}")
        with col_eq2:
            st.info(f"**Type:** {equipment_info.get('equipment_type', 'Unknown')}")
        with col_eq3:
            st.info(f"**Manufacturer:** {equipment_info.get('manufacturer', 'Unknown')}")
        with col_eq4:
            st.info(f"**Series:** {equipment_info.get('series', 'Unknown')}")

    # ğŸ†• === AFFICHAGE 4 COLONNES ===
    st.markdown("---")
    mode_title = "Multi-Query Fusion" if use_llm_preprocessing else "Mode Classique"
    st.markdown(f"## ğŸ“Š Comparaison des 4 systÃ¨mes - {mode_title}")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.markdown("### ğŸ“˜ RAG Classique")
        st.markdown("*BM25 + FAISS + CrossEncoder*")
        st.success(classic_result["answer"])
        st.caption(f"â±ï¸ {classic_result['time']:.2f}s | ğŸ“„ {classic_result['docs_count']} docs")
        
        # StratÃ©gie
        strategy = classic_result["strategy"]
        st.markdown(f"<p style='color: {strategy['color']}; font-size: 0.8em;'>{strategy['text']}</p>", 
                   unsafe_allow_html=True)
    
    with col2:
        st.markdown("### ğŸ§  RAG + KG Dense")
        mode_str = "Multi-Query + Equipment" if use_llm_preprocessing else "Single-Query + Equipment"
        st.markdown(f"*{mode_str}*")
        st.success(dense_result["answer"])
        st.caption(f"â±ï¸ {dense_result['time']:.2f}s | ğŸ“„ {dense_result['docs_count']} docs")
        
        # StratÃ©gie
        strategy = dense_result["strategy"]
        st.markdown(f"<p style='color: {strategy['color']}; font-size: 0.8em;'>{strategy['text']}</p>", 
                   unsafe_allow_html=True)
    
    with col3:
        st.markdown("### ğŸŸ¤ RAG + KG Sparse")
        mode_str = "Multi-Query + Equipment" if use_llm_preprocessing else "Single-Query + Equipment"
        st.markdown(f"*{mode_str}*")
        st.success(sparse_result["answer"])
        st.caption(f"â±ï¸ {sparse_result['time']:.2f}s | ğŸ“„ {sparse_result['docs_count']} docs")
        
        # StratÃ©gie
        strategy = sparse_result["strategy"]
        st.markdown(f"<p style='color: {strategy['color']}; font-size: 0.8em;'>{strategy['text']}</p>", 
                   unsafe_allow_html=True)
    
    with col4:
        st.markdown("### ğŸ”¶ RAG + KG Dense S&C")
        mode_str = "Multi-Query + Equipment" if use_llm_preprocessing else "Single-Query + Equipment"
        st.markdown(f"*{mode_str}*")
        st.success(dense_sc_result["answer"])
        st.caption(f"â±ï¸ {dense_sc_result['time']:.2f}s | ğŸ“„ {dense_sc_result['docs_count']} docs")
        
        # StratÃ©gie
        strategy = dense_sc_result["strategy"]
        st.markdown(f"<p style='color: {strategy['color']}; font-size: 0.8em;'>{strategy['text']}</p>", 
                   unsafe_allow_html=True)

    # ğŸ†• === Ã‰VALUATION LLM JUGE 4 RÃ‰PONSES ===
    if response_evaluator:
        st.markdown("---")
        st.markdown("## ğŸ¤– Analyse LLM Juge - 4 SystÃ¨mes")
        
        with st.spinner("ğŸ” Ã‰valuation comparative des 4 rÃ©ponses..."):
            try:
                eval_result = response_evaluator.evaluate_4_responses(
                    query,
                    classic_result["answer"],
                    dense_result["answer"],
                    sparse_result["answer"],
                    dense_sc_result["answer"]
                )
                
                # Affichage des scores
                col_eval1, col_eval2, col_eval3, col_eval4 = st.columns(4)
                
                with col_eval1:
                    score = eval_result.get("score_classic", 0)
                    st.metric("ğŸ“˜ Score Classique", f"{score:.1f}/5")
                
                with col_eval2:
                    score = eval_result.get("score_dense", 0)
                    st.metric("ğŸ§  Score Dense", f"{score:.1f}/5")
                
                with col_eval3:
                    score = eval_result.get("score_sparse", 0)
                    st.metric("ğŸŸ¤ Score Sparse", f"{score:.1f}/5")
                
                with col_eval4:
                    score = eval_result.get("score_dense_sc", 0)
                    st.metric("ğŸ”¶ Score Dense S&C", f"{score:.1f}/5")
                
                # Analyse comparative
                if "comparative_analysis" in eval_result:
                    st.info(f"**Analyse :** {eval_result['comparative_analysis']}")
                
                if "best_approach" in eval_result:
                    st.success(f"**Recommandation :** {eval_result['best_approach']}")
                    
            except Exception as e:
                st.warning(f"âš ï¸ Ã‰valuation indisponible: {e}")

    # === MÃ‰TRIQUES DE PERFORMANCE ===
    st.markdown("---")
    st.markdown("## â±ï¸ MÃ©triques de performance")
    
    col3, col4, col5, col6 = st.columns(4)
    
    all_times = [classic_result["time"], dense_result["time"], sparse_result["time"], dense_sc_result["time"]]
    
    with col3:
        st.metric("ğŸƒ Plus Rapide", f"{min(all_times):.2f}s")
    
    with col4:
        st.metric("âš¡ Temps Total", f"{sum(all_times):.2f}s")
    
    with col5:
        st.metric("ğŸŒ Mode", "Cloud" if cloud_enabled else "Local")
    
    with col6:
        multi_query_status = "âœ… Multi-Query" if use_llm_preprocessing else "âŒ Classique"
        st.metric("ğŸ§  Mode", multi_query_status)

    # === ğŸ†• AFFICHAGE CONDITIONNEL DES DÃ‰TAILS TECHNIQUES ENRICHI ===
    if show_details:
        st.markdown("---")
        st.markdown("## ğŸ” DÃ©tails techniques - Contexte LLM complet")
        
        # Tabs pour les 4 systÃ¨mes
        tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“˜ Classique", "ğŸ§  Dense", "ğŸŸ¤ Sparse", "ğŸ”¶ Dense S&C"])
        
        for tab, result, system_name in zip([tab1, tab2, tab3, tab4], 
                                           [classic_result, dense_result, sparse_result, dense_sc_result],
                                           ["Classique", "Dense", "Sparse", "Dense S&C"]):
            with tab:
                # ğŸ†• AFFICHAGE DE LA QUERY UTILISÃ‰E (pour debug)
                st.markdown(f"### ğŸ¯ Query utilisÃ©e par le gÃ©nÃ©rateur {system_name}")
                effective_query = result.get("effective_query", "N/A")
                if effective_query == query:
                    st.info(f"**Query originale utilisÃ©e:** {effective_query}")
                else:
                    st.success(f"**Query filtrÃ©e utilisÃ©e:** {effective_query}")
                    st.caption(f"*Query originale:* {query}")
                
                # ğŸ†• Info Multi-Query si applicable
                if result.get("processed_query") and use_llm_preprocessing and system_name != "Classique":
                    st.markdown(f"### ğŸ§  Multi-Query Info - {system_name}")
                    pq = result["processed_query"]
                    st.info(f"Query filtrÃ©e: {pq.filtered_query}")
                    st.info(f"Variantes: {', '.join(pq.query_variants[:2])}...")
                
                # ğŸ†• AFFICHAGE DU CONTEXTE DOCUMENTAIRE ENVOYÃ‰ AU LLM
                st.markdown(f"### ğŸ“„ Contexte documentaire envoyÃ© au LLM {system_name}")
                document_context = result.get("document_context", "")
                if document_context and document_context.strip():
                    with st.expander(f"Contexte documentaire ({len(document_context)} caractÃ¨res)"):
                        st.text_area("Contexte documentaire", value=document_context, height=200, 
                                        key=f"doc_context_{system_name}", label_visibility="hidden")
                else:
                    st.warning("Aucun contexte documentaire")
                
                # ğŸ†• AFFICHAGE DU CONTEXTE KG ENVOYÃ‰ AU LLM (si applicable)
                if system_name != "Classique":
                    st.markdown(f"### ğŸ§  Contexte Knowledge Graph envoyÃ© au LLM {system_name}")
                    kg_context = result.get("kg_context", "")
                    if kg_context and kg_context.strip() and not kg_context.startswith("[Pas de contexte"):
                        mode_info = " (Multi-Query)" if use_llm_preprocessing else " (Single-Query)"
                        with st.expander(f"Contexte KG {system_name}{mode_info} ({len(kg_context)} caractÃ¨res)"):
                            st.text_area("Contexte Knowledge Graph", value=kg_context, height=300, 
                                        key=f"kg_context_{system_name}", label_visibility="hidden")
                    else:
                        st.warning(f"Aucun contexte KG pertinent pour {system_name}")
                
                # Documents dÃ©taillÃ©s (section existante conservÃ©e)
                st.markdown(f"### ğŸ“Š Documents rÃ©cupÃ©rÃ©s - {system_name}")
                for i, doc in enumerate(result["reranked_docs"]):
                    score = doc.get('score', 'N/A')
                    with st.expander(f"Document #{i+1} - {doc.get('source', 'Unknown')} (Score: {score:.3f})"):
                        st.markdown(doc['content'])
                
                # Triplets KG dÃ©taillÃ©s (section existante conservÃ©e)
                if result["kg_triplets"] and system_name != "Classique":
                    st.markdown(f"### ğŸ”— Triplets KG dÃ©taillÃ©s - {system_name}")
                    mode_info = " (Multi-Query)" if use_llm_preprocessing else " (Single-Query)"
                    with st.expander(f"Triplets extraits du KG{mode_info}"):
                        st.text(result["kg_triplets"])

# === SIDEBAR AVEC INFORMATIONS (CONSERVÃ‰E + ENRICHIE) ===
with st.sidebar:
    st.markdown("## â„¹ï¸ Comparateur RAG 4 SystÃ¨mes")
    
    if use_llm_preprocessing:
        st.markdown("### ğŸ†• Multi-Query Fusion ACTIVÃ‰")
        st.markdown("""
        **ğŸ§  LLM Preprocessing + Multi-Query :**
        - Analyse LLM intelligente de la requÃªte
        - Extraction de termes techniques
        - ğŸ­ Identification + Matching Ã©quipement
        - ğŸ”„ GÃ©nÃ©ration de variantes de requÃªte
        - ğŸ“Š Recherche multi-variantes sur KG
        - âœ‚ï¸ DÃ©duplication intelligente
        - ğŸ¯ Fusion avec stratÃ©gie MAX Score
        - ğŸ”§ Filtrage KG par Ã©quipement dÃ©tectÃ©
        """)
    else:
        st.markdown("### ğŸ“„ Mode Classique ACTIVÃ‰")
        st.markdown("""
        **ğŸ“„ Recherche classique :**
        - Recherche directe sur la requÃªte
        - BM25 + FAISS + CrossEncoder
        - Equipment Matching disponible
        - Single-Query sur les KG
        """)
    
    st.markdown("""
    **4 SystÃ¨mes ComparÃ©s :**
    
    ğŸ“˜ **RAG Classique**
    - BM25 (recherche lexicale)
    - FAISS (recherche sÃ©mantique)
    - Fusion + CrossEncoder
    - GÃ©nÃ©ration LLM
    
    ğŸ§  **RAG + KG Dense**
    - Recherche vectorielle + hybride
    - Propagation sÃ©mantique
    - TraversÃ©e SCR enrichie
    - Multi-Query + Equipment Matching
    
    ğŸŸ¤ **RAG + KG Sparse**
    - Structure 1:1:1 directe
    - TraÃ§abilitÃ© parfaite
    - Pas de propagation
    - Multi-Query + Equipment Matching
    
    ğŸ”¶ **RAG + KG Dense S&C**
    - SymptÃ´me + Cause combinÃ©
    - Enrichissement contextuel
    - Propagation sÃ©mantique S&C
    - Multi-Query + Equipment Matching
    """)
    
    st.markdown("## ğŸ¤– LLM Juge 4 RÃ©ponses")
    st.markdown("""
    **Ã‰valuation automatique :**
    - Note individuelle (0-5) par systÃ¨me
    - PrÃ©cision technique
    - ComplÃ©tude de la rÃ©ponse
    - ClartÃ© pour ouvriers
    - ConsidÃ©rations sÃ©curitÃ©
    
    **Comparaison intelligente** des 4 approches RAG.
    """)
    
    if use_llm_preprocessing:
        st.markdown("## ğŸ§  Multi-Query Fusion")
        st.markdown("""
        **Fonctionnement :**
        - LLM analyse et optimise la requÃªte
        - GÃ©nÃ©ration de 2 variantes intelligentes
        - Recherche parallÃ¨le sur KG avec toutes les variantes
        - Fusion avec stratÃ©gie MAX Score
        - DÃ©duplication automatique
        - SÃ©lection des top 3 triplets optimaux
        
        **Avantage :** Couverture sÃ©mantique Ã©largie
        """)
    
    st.markdown("## ğŸ­ Equipment Matching")
    st.markdown("""
    **Fonctionnement :**
    - LLM extrait equipment de la requÃªte
    - Cosine similarity avec KG equipment
    - Si match > 0.9 : filtrage ciblÃ© KG
    - Sinon : recherche globale KG
    
    **Optimisation :** Recherche plus prÃ©cise selon l'Ã©quipement identifiÃ©.
    """)
    
    if cloud_enabled:
        st.markdown("## ğŸŒ Status Cloud")
        st.success("Neo4j Cloud Actif")
        st.markdown("*Fallback automatique vers local si erreur*")
    else:
        st.markdown("## ğŸ  Status Local")
        st.info("Neo4j Local Actif")
    
    if use_llm_preprocessing and query_processor:
        st.markdown("## ğŸ§  LLM Configuration")
        try:
            llm_config = query_processor.get_config() if query_processor else {}
            st.markdown(f"""
            **Configuration active :**
            - ModÃ¨le : `{llm_config.get('llm_config', {}).get('model', 'N/A')}`
            - Tokens max : `{llm_config.get('llm_config', {}).get('max_tokens', 'N/A')}`
            - TempÃ©rature : `{llm_config.get('llm_config', {}).get('temperature', 'N/A')}`
            - Multi-Query : âœ… ACTIVÃ‰
            """)
        except:
            st.markdown("*Configuration non disponible*")
    
    # ğŸ†• Status Multi-Query
    st.markdown("---")
    if use_llm_preprocessing:
        st.success("ğŸš€ Multi-Query Fusion ACTIVÃ‰")
        st.markdown("Les 3 KG utilisent la recherche Multi-Query")
    else:
        st.info("ğŸ“„ Mode Single-Query")
        st.markdown("Recherche classique sur les KG")
    
    # ğŸ†• INFO DEBUG
    if show_details:
        st.markdown("---")
        st.markdown("## ğŸ” Mode Debug Actif")
        st.markdown("""
        **DÃ©tails techniques affichÃ©s :**
        - ğŸ¯ Query utilisÃ©e par chaque gÃ©nÃ©rateur
        - ğŸ“„ Contexte documentaire envoyÃ© au LLM
        - ğŸ§  Contexte KG envoyÃ© au LLM
        - ğŸ“Š Documents rÃ©cupÃ©rÃ©s dÃ©taillÃ©s
        - ğŸ”— Triplets KG extraits
        """)
        st.info("Ces informations vous permettent de vÃ©rifier que la query filtrÃ©e est bien utilisÃ©e et de voir exactement le contexte envoyÃ© aux LLM gÃ©nÃ©rateurs.")