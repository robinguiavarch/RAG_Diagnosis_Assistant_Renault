"""
Client LLM amÃ©liorÃ© pour l'Ã©valuation des rÃ©ponses
Version avec vÃ©rification de cohÃ©rence et paramÃ¨tres optimisÃ©s
"""

import os
import yaml
import json
from dotenv import load_dotenv
from openai import OpenAI
from difflib import SequenceMatcher
from typing import List

load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))


class LLMJudgeClient:
    """Client LLM juge avec vÃ©rification de cohÃ©rence"""
    
    def __init__(self):
        # Chargement config depuis settings.yaml
        try:
            with open("config/settings.yaml", "r") as f:
                settings = yaml.safe_load(f)
            
            judge_cfg = settings.get("evaluation", {}).get("llm_judge", {})
            self.model = judge_cfg.get("model", "gpt-4o")  # ğŸ”§ UPGRADE par dÃ©faut
            self.temperature = judge_cfg.get("temperature", 0.0)  # ğŸ”§ FIX dÃ©terminisme
            self.max_tokens = judge_cfg.get("max_tokens", 500)  # ğŸ”§ INCREASE
            
            # ğŸ†• NOUVEAUX PARAMÃˆTRES pour cohÃ©rence
            self.seed = judge_cfg.get("seed", 42)
            self.retry_on_inconsistency = judge_cfg.get("retry_on_inconsistency", True)
            self.max_retries = judge_cfg.get("max_retries", 2)
            self.similarity_threshold = judge_cfg.get("similarity_threshold", 0.9)
            self.max_score_difference = judge_cfg.get("max_score_difference", 0.3)
            
            print(f"ğŸ¯ LLMJudgeClient initialisÃ©:")
            print(f"   ğŸ¤– ModÃ¨le: {self.model}")
            print(f"   ğŸŒ¡ï¸ Temperature: {self.temperature}")
            print(f"   ğŸ“ Max tokens: {self.max_tokens}")
            print(f"   ğŸ”„ Retry incohÃ©rence: {self.retry_on_inconsistency}")
            
        except FileNotFoundError:
            # Config par dÃ©faut amÃ©liorÃ©e
            print("âš ï¸ settings.yaml non trouvÃ©, utilisation config par dÃ©faut amÃ©liorÃ©e")
            self.model = "gpt-4o"
            self.temperature = 0.0
            self.max_tokens = 500
            self.seed = 42
            self.retry_on_inconsistency = True
            self.max_retries = 2
            self.similarity_threshold = 0.9
            self.max_score_difference = 0.3
    
    def similarity(self, text1: str, text2: str) -> float:
        """Calcule la similaritÃ© entre deux textes"""
        return SequenceMatcher(None, text1.lower().strip(), text2.lower().strip()).ratio()
    
    def extract_responses_from_prompt(self, prompt: str) -> List[str]:
        """Extrait les 4 rÃ©ponses depuis le prompt pour vÃ©rification similaritÃ©"""
        try:
            responses = []
            lines = prompt.split('\n')
            current_response = ""
            capturing = False
            
            for line in lines:
                if "RESPONSE" in line and any(keyword in line for keyword in ["RAG", "CLASSIQUE", "DENSE", "SPARSE"]):
                    if current_response and capturing:
                        responses.append(current_response.strip())
                    current_response = ""
                    capturing = True
                    continue
                elif capturing and (line.startswith("RESPONSE") or "Provide JSON" in line or "EVALUATION CRITERIA" in line):
                    if current_response:
                        responses.append(current_response.strip())
                    if "Provide JSON" in line:
                        break
                    current_response = ""
                    capturing = True
                elif capturing and line.strip():
                    current_response += line.strip() + " "
            
            # Ajouter la derniÃ¨re rÃ©ponse si nÃ©cessaire
            if current_response and capturing:
                responses.append(current_response.strip())
            
            return responses[:4]  # Max 4 rÃ©ponses
            
        except Exception as e:
            print(f"âš ï¸ Erreur extraction rÃ©ponses: {e}")
            return []
    
    def check_response_consistency(self, responses: List[str], scores: List[float]) -> bool:
        """VÃ©rifie la cohÃ©rence entre rÃ©ponses similaires et leurs scores"""
        if len(responses) != 4 or len(scores) != 4:
            return True  # Skip si pas 4 rÃ©ponses
            
        for i in range(len(responses)):
            for j in range(i + 1, len(responses)):
                similarity_score = self.similarity(responses[i], responses[j])
                
                if similarity_score >= self.similarity_threshold:
                    score_diff = abs(scores[i] - scores[j])
                    if score_diff > self.max_score_difference:
                        print(f"âš ï¸ IncohÃ©rence dÃ©tectÃ©e:")
                        print(f"   SimilaritÃ© rÃ©ponses {i+1}-{j+1}: {similarity_score:.2f}")
                        print(f"   DiffÃ©rence scores: {score_diff:.2f} > seuil {self.max_score_difference}")
                        print(f"   Scores: {scores[i]:.1f} vs {scores[j]:.1f}")
                        return False
        return True
    
    def parse_evaluation_for_check(self, llm_response: str) -> dict:
        """Parse rapide pour vÃ©rification cohÃ©rence"""
        try:
            import re
            json_pattern = r'```json\s*(\{.*?\})\s*```'
            json_match = re.search(json_pattern, llm_response, re.DOTALL | re.IGNORECASE)
            
            if json_match:
                json_str = json_match.group(1)
            else:
                json_pattern = r'(\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\})'
                json_match = re.search(json_pattern, llm_response, re.DOTALL)
                if json_match:
                    json_str = json_match.group(1)
                else:
                    return {}
            
            return json.loads(json_str)
        except Exception:
            return {}
    
    def evaluate(self, prompt: str) -> str:
        """
        Ã‰value avec vÃ©rification de cohÃ©rence et retry si nÃ©cessaire
        
        Args:
            prompt: Prompt avec les rÃ©ponses Ã  Ã©valuer
            
        Returns:
            str: RÃ©ponse JSON du LLM
        """
        for attempt in range(self.max_retries + 1):
            try:
                # Appel LLM avec paramÃ¨tres dÃ©terministes optimisÃ©s
                response = client.chat.completions.create(
                    model=self.model,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=self.temperature,
                    max_tokens=self.max_tokens,
                    seed=self.seed,  # ğŸ†• DÃ©terminisme
                    top_p=1.0,      # ğŸ†• Pas de nucleus sampling
                    frequency_penalty=0.0,  # ğŸ†• Pas de pÃ©nalitÃ©
                    presence_penalty=0.0    # ğŸ†• Pas de pÃ©nalitÃ©
                )
                
                llm_response = response.choices[0].message.content.strip()
                
                # ğŸ†• VÃ‰RIFICATION DE COHÃ‰RENCE si activÃ©e
                if self.retry_on_inconsistency and attempt < self.max_retries:
                    try:
                        # Parse l'Ã©valuation
                        evaluation = self.parse_evaluation_for_check(llm_response)
                        
                        # Extraction des scores
                        scores = [
                            evaluation.get("score_classic", 0),
                            evaluation.get("score_dense", 0),
                            evaluation.get("score_sparse", 0),
                            evaluation.get("score_dense_sc", 0)
                        ]
                        
                        # Extraction des rÃ©ponses depuis le prompt
                        responses = self.extract_responses_from_prompt(prompt)
                        
                        # VÃ©rification cohÃ©rence
                        if len(responses) == 4 and len(scores) == 4:
                            if self.check_response_consistency(responses, scores):
                                print(f"âœ… Ã‰valuation cohÃ©rente (tentative {attempt + 1})")
                                return llm_response
                            else:
                                print(f"ğŸ”„ Retry {attempt + 1}/{self.max_retries} - IncohÃ©rence dÃ©tectÃ©e")
                                continue
                        else:
                            # Si on ne peut pas vÃ©rifier, on accepte
                            return llm_response
                            
                    except Exception as e:
                        print(f"âš ï¸ Erreur vÃ©rification cohÃ©rence: {e}")
                        return llm_response
                else:
                    # DerniÃ¨re tentative ou vÃ©rification dÃ©sactivÃ©e
                    return llm_response
                
            except Exception as e:
                if attempt == self.max_retries:
                    error_response = json.dumps({
                        "error": f"Erreur LLM juge aprÃ¨s {self.max_retries + 1} tentatives: {str(e)}",
                        "score_classic": 2.5,
                        "score_dense": 2.5,
                        "score_sparse": 2.5,
                        "score_dense_sc": 2.5,
                        "best_approach": "Ã‰valuation indisponible (erreur technique)",
                        "comparative_analysis": "Erreur lors de l'Ã©valuation automatique"
                    })
                    return error_response
                
                print(f"âŒ Tentative {attempt + 1} Ã©chouÃ©e: {e}")
                continue
        
        # Fallback (ne devrait jamais arriver)
        return '{"error": "Erreur inattendue dans l\'Ã©valuateur"}'


# Fonction utilitaire (rÃ©trocompatibilitÃ©)
def create_judge_client() -> LLMJudgeClient:
    """CrÃ©e un client juge amÃ©liorÃ©"""
    return LLMJudgeClient()