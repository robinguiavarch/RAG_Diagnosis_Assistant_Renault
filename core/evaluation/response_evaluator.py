"""
√âvaluateur de r√©ponses - Version compl√®te avec support 4 r√©ponses
Path: core/evaluation/response_evaluator.py
"""

import json
import re
from pathlib import Path
from .llm_judge_client import LLMJudgeClient


class ResponseEvaluator:
    """√âvaluateur pour comparer 2 ou 4 r√©ponses RAG simultan√©ment"""
    
    def __init__(self):
        self.judge_client = LLMJudgeClient()
        self.prompt_template = self._load_prompt()
    
    def _load_prompt(self) -> str:
        """Charge le prompt d'√©valuation depuis le fichier externalis√©"""
        try:
            prompt_path = Path("config/prompts/judge_evaluation_prompt.txt")
            with open(prompt_path, 'r', encoding='utf-8') as f:
                return f.read()
        except FileNotFoundError:
            # Prompt minimal par d√©faut pour 2 r√©ponses
            return """Compare these 2 responses and give scores 0-5:

QUERY: {query}

RESPONSE 1: {response1}

RESPONSE 2: {response2}

Return JSON: {{"score_response_1": X.X, "score_response_2": X.X, "comparative_justification": "brief explanation"}}"""
    
    def evaluate_responses(self, query: str, response1: str, response2: str) -> dict:
        """
        √âvalue et compare 2 r√©ponses (fonction originale conserv√©e)
        
        Args:
            query: Question utilisateur
            response1: Premi√®re r√©ponse (RAG classique)
            response2: Deuxi√®me r√©ponse (RAG enrichi)
            
        Returns:
            dict: {"score_response_1": float, "score_response_2": float, "comparative_justification": str}
        """
        # Construction du prompt pour 2 r√©ponses
        prompt = self.prompt_template.format(
            query=query,
            response1=response1,
            response2=response2
        )
        
        # Appel LLM
        llm_response = self.judge_client.evaluate(prompt)
        
        # Parse JSON
        try:
            return self._parse_evaluation(llm_response)
        except:
            # Fallback en cas d'erreur
            return {
                "score_response_1": 2.5,
                "score_response_2": 2.5,
                "comparative_justification": "Erreur de parsing de l'√©valuation"
            }
    
    def evaluate_4_responses(self, query: str, response_classic: str, response_dense: str, 
                            response_sparse: str, response_dense_sc: str) -> dict:
        """
        üÜï NOUVELLE FONCTION: √âvalue et compare 4 r√©ponses RAG simultan√©ment
        Utilise le prompt externalis√© judge_evaluation_prompt.txt
        
        Args:
            query: Question utilisateur
            response_classic: R√©ponse RAG Classique
            response_dense: R√©ponse RAG + KG Dense
            response_sparse: R√©ponse RAG + KG Sparse
            response_dense_sc: R√©ponse RAG + KG Dense S&C
            
        Returns:
            dict: Scores et analyse comparative des 4 approches
        """
        # Construction du prompt 4 r√©ponses avec le template externalis√©
        prompt = self.prompt_template.format(
            query=query,
            response_classic=response_classic,
            response_dense=response_dense,
            response_sparse=response_sparse,
            response_dense_sc=response_dense_sc
        )
        
        # Appel LLM
        llm_response = self.judge_client.evaluate(prompt)
        
        # Parse JSON
        try:
            return self._parse_evaluation(llm_response)
        except Exception as e:
            print(f"‚ùå Erreur parsing √©valuation 4 r√©ponses: {e}")
            # Fallback en cas d'erreur
            return {
                "score_classic": 2.5,
                "score_dense": 2.5,
                "score_sparse": 2.5,
                "score_dense_sc": 2.5,
                "best_approach": "√âvaluation indisponible (erreur de parsing)",
                "comparative_analysis": "Erreur lors de l'√©valuation automatique des r√©ponses"
            }
    
    def _parse_evaluation(self, llm_response: str) -> dict:
        """Parse la r√©ponse JSON du LLM (compatible 2 et 4 r√©ponses)"""
        # Extraction du JSON (m√™me logique que preprocessing)
        json_pattern = r'```json\s*(\{.*?\})\s*```'
        json_match = re.search(json_pattern, llm_response, re.DOTALL | re.IGNORECASE)
        
        if json_match:
            json_str = json_match.group(1)
        else:
            # Fallback: chercher JSON direct
            json_pattern = r'(\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\})'
            json_match = re.search(json_pattern, llm_response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                raise Exception("Pas de JSON trouv√© dans la r√©ponse LLM")
        
        return json.loads(json_str)


def create_response_evaluator() -> ResponseEvaluator:
    """Fonction utilitaire pour cr√©er un √©valuateur"""
    return ResponseEvaluator()


# === FONCTION UTILITAIRE POUR TEST RAPIDE ===
def evaluate_responses_quick(query: str, response1: str, response2: str) -> dict:
    """Fonction utilitaire pour √©valuation rapide de 2 r√©ponses"""
    evaluator = create_response_evaluator()
    return evaluator.evaluate_responses(query, response1, response2)


def evaluate_4_responses_quick(query: str, response_classic: str, response_dense: str, 
                              response_sparse: str, response_dense_sc: str) -> dict:
    """Fonction utilitaire pour √©valuation rapide de 4 r√©ponses"""
    evaluator = create_response_evaluator()
    return evaluator.evaluate_4_responses(query, response_classic, response_dense, response_sparse, response_dense_sc)


if __name__ == "__main__":
    # Test simple de l'√©valuateur 4 r√©ponses
    test_query = "ACAL-006 error on FANUC R-30iB teach pendant"
    
    test_responses = {
        "classic": "Error ACAL-006 indicates a calibration issue. Check teach pendant connection.",
        "dense": "ACAL-006 is a calibration error on FANUC R-30iB. Based on knowledge graph: symptom indicates TPE communication failure. Remedy: restart controller and recalibrate.",
        "sparse": "Error ACAL-006: Direct mapping shows teach pendant operation error. Solution: Check cables and restart system.",
        "dense_sc": "ACAL-006 represents calibration failure (enriched with cause analysis). The symptom-cause relationship indicates TPE hardware issue. Recommended remedy: hardware check then recalibration."
    }
    
    try:
        evaluator = create_response_evaluator()
        result = evaluator.evaluate_4_responses(
            test_query,
            test_responses["classic"],
            test_responses["dense"], 
            test_responses["sparse"],
            test_responses["dense_sc"]
        )
        
        print("üß™ Test √©valuation 4 r√©ponses:")
        print(f"   Classic: {result.get('score_classic', 'N/A')}")
        print(f"   Dense: {result.get('score_dense', 'N/A')}")
        print(f"   Sparse: {result.get('score_sparse', 'N/A')}")
        print(f"   Dense S&C: {result.get('score_dense_sc', 'N/A')}")
        print(f"   Meilleure approche: {result.get('best_approach', 'N/A')}")
        print("‚úÖ Test r√©ussi")
        
    except Exception as e:
        print(f"‚ùå Test √©chou√©: {e}")