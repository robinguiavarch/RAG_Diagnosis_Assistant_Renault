"""
Client LLM simple pour le prÃ©processing des requÃªtes
Focus OpenAI uniquement - simplicitÃ© maximale
ğŸ”§ CORRECTION: Chargement settings.yaml AVANT initialisation par dÃ©faut
"""

import os
import yaml
from typing import Dict, Any, Optional
from dotenv import load_dotenv
from openai import OpenAI

load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))


def _load_llm_config() -> Dict[str, Any]:
    """
    ğŸ†• Charge la configuration LLM depuis settings.yaml EN PREMIER
    Retourne les paramÃ¨tres par dÃ©faut si fichier absent
    """
    default_config = {
        "model": "gpt-4o-mini",  # Fallback si settings.yaml absent
        "max_tokens": 1000,
        "temperature": 0.1
    }
    
    try:
        with open("config/settings.yaml", "r") as f:
            settings = yaml.safe_load(f)
        
        llm_cfg = settings.get("query_processing", {}).get("llm", {})
        
        # Merge avec dÃ©fauts (prioritÃ© aux settings.yaml)
        config = {
            "model": llm_cfg.get("model", default_config["model"]),
            "max_tokens": llm_cfg.get("max_tokens", default_config["max_tokens"]),
            "temperature": llm_cfg.get("temperature", default_config["temperature"])
        }
        
        print(f"ğŸ”§ Configuration LLM chargÃ©e depuis settings.yaml:")
        print(f"   ğŸ“ ModÃ¨le: {config['model']}")
        print(f"   ğŸ›ï¸ Temperature: {config['temperature']}")
        print(f"   ğŸ“Š Max tokens: {config['max_tokens']}")
        
        return config
        
    except FileNotFoundError:
        print("âš ï¸ settings.yaml non trouvÃ©, utilisation configuration par dÃ©faut")
        return default_config
    except Exception as e:
        print(f"âš ï¸ Erreur lecture settings.yaml: {e}, utilisation configuration par dÃ©faut")
        return default_config


class LLMClient:
    """Client simplifiÃ© pour OpenAI avec chargement prioritaire settings.yaml"""
    
    def __init__(self, model: Optional[str] = None, max_tokens: Optional[int] = None, 
                 temperature: Optional[float] = None):
        """
        ğŸ”§ CORRECTION: Charge settings.yaml AVANT d'appliquer les paramÃ¨tres
        
        Args:
            model: ModÃ¨le Ã  utiliser (override settings.yaml si fourni)
            max_tokens: Tokens max (override settings.yaml si fourni)
            temperature: TempÃ©rature (override settings.yaml si fourni)
        """
        # ğŸ†• CHARGEMENT PRIORITAIRE depuis settings.yaml
        config = _load_llm_config()
        
        # Application des paramÃ¨tres (prioritÃ©: paramÃ¨tres explicites > settings.yaml > dÃ©fauts)
        self.model = model if model is not None else config["model"]
        self.max_tokens = max_tokens if max_tokens is not None else config["max_tokens"]
        self.temperature = temperature if temperature is not None else config["temperature"]
        
        print(f"âœ… LLMClient initialisÃ©:")
        print(f"   ğŸ¤– ModÃ¨le final: {self.model}")
        print(f"   ğŸŒ¡ï¸ TempÃ©rature finale: {self.temperature}")
        print(f"   ğŸ“ Max tokens final: {self.max_tokens}")
    
    def generate(self, prompt: str) -> str:
        """
        GÃ©nÃ¨re une rÃ©ponse pour le prompt donnÃ©
        
        Args:
            prompt: Le prompt Ã  envoyer au LLM
            
        Returns:
            str: RÃ©ponse du LLM
        """
        try:
            print(f"ğŸ§  Appel LLM {self.model} avec {len(prompt)} caractÃ¨res de prompt")
            
            response = client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=self.temperature,
                max_tokens=self.max_tokens
            )
            
            llm_response = response.choices[0].message.content.strip()
            print(f"âœ… RÃ©ponse LLM reÃ§ue: {len(llm_response)} caractÃ¨res")
            
            return llm_response
            
        except Exception as e:
            print(f"âŒ Erreur LLM: {e}")
            raise Exception(f"Erreur LLM: {e}")
    
    def get_config(self) -> Dict[str, Any]:
        """Retourne la configuration actuelle"""
        return {
            "model": self.model,
            "max_tokens": self.max_tokens,
            "temperature": self.temperature
        }


# Fonction utilitaire pour usage rapide
def create_llm_client(model: Optional[str] = None) -> LLMClient:
    """
    CrÃ©e un client LLM avec configuration prioritaire settings.yaml
    
    Args:
        model: Override le modÃ¨le depuis settings.yaml si fourni
    """
    return LLMClient(model=model)


if __name__ == "__main__":
    # Test simple avec affichage de la config
    print("ğŸ§ª Test LLMClient avec settings.yaml")
    
    client = create_llm_client()
    
    test_prompt = "Explain what ACAL-006 error means in one sentence."
    try:
        response = client.generate(test_prompt)
        print(f"âœ… Test rÃ©ussi: {response}")
    except Exception as e:
        print(f"âŒ Test Ã©chouÃ©: {e}")